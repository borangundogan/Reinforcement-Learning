{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117eb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53957cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_dim):\n",
    "            self.mem_size = max_size\n",
    "            self.mem_counter = 0\n",
    "        \n",
    "            self.state_memory = np.zeros((self.mem_size, *input_dim), dtype= np.float32)\n",
    "            self.new_state_memory = np.zeros((self.mem_size, *input_dim), dtype= np.float32)\n",
    "        \n",
    "            self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "            self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "            self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "            index = self.mem_counter % self.mem_size\n",
    "            self.state_memory[index] = state\n",
    "            self.new_state_memory[index] = _state\n",
    "            self.reward_memory[index] = reward\n",
    "            self.action_memory[index] = action\n",
    "            self.terminal_memory[index] = 1 - int(done)\n",
    "            self.mem_counter += 1\n",
    "    \n",
    "    def sample_buffer(self, batch_size):\n",
    "            max_mem = min(self.mem_counter, self.mem_size)\n",
    "            batch = np.random.choice(max_mem, batch_size, replace = False)\n",
    "            states = self.state_memory[batch]\n",
    "            states_ = self.new_state_memory[batch]\n",
    "            reward = self.reward_memory[batch]\n",
    "            action = self.action_memory[batch]\n",
    "            terminal = self.terminal_memory[batch]\n",
    "            \n",
    "            return states, states_ , action, reward , terminal\n",
    "    \n",
    "    def build_dqn(lr, n_actions, input_dims, fc1_dims, fc2_dims):\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(fc1_dims, activation=\"relu\"),\n",
    "            keras.layers.Dense(fc2_dims, activation=\"relu\"),\n",
    "            keras.layers.Dense(n_actions, activation=None)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate = lr), loss = \"mean_squared_error\")\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e68b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, lr, gamma, n_actions, epsilon, batch_size, input_dims,\n",
    "                epsilon_dec = 1e-3, epsilon_end = 0.01, mem_size = 1000000, fname = \"dqn_model.h5\"):\n",
    "            self.action = [i for i in range(n_actions)]\n",
    "            self.gamma = gamma\n",
    "            self.epsilon = epsilon\n",
    "            self.eps_min = epsilon_end\n",
    "            self.batch_size = batch_size\n",
    "            self.model_file = fname\n",
    "            self.memory = ReplayBuffer(mem_size, input_dims)\n",
    "            self.q_eval = build_dqn(lr, n_actions, input_dims, 256, 256)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state,done)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "            if np.random.random() < self.epsilon: \n",
    "                action = np.random.choice(self.action_space)\n",
    "            else : \n",
    "                state = np.array([observation])\n",
    "                actions = self.q_eval.predict(state)\n",
    "                \n",
    "                action = np.argmax(actions)\n",
    "            return action\n",
    "    \n",
    "    def learn(self):\n",
    "            if self.memory.mem_counter < self.batch_size:\n",
    "                return \n",
    "            states, states_, actions, rewards, dones = \\\n",
    "                    self.memory.sample_buffer(batch_size)\n",
    "            q_eval = self.q_eval.predict(states)\n",
    "            q_next = self.q_eval.predict(states_)\n",
    "            \n",
    "            q_target = np.copy(q_eval)\n",
    "            batch_index = np.arrange(self.batch_size , dtype=np.int32)\n",
    "            \n",
    "            q_target[batch_index, actions] = rewards + \\\n",
    "                                self.gamma* np.max(q_next, axis=1) * dones\n",
    "            \n",
    "            self.q_eval.train_on_batch(states, q_target)\n",
    "            \n",
    "            self.epsilon = self.epsilon - self.eps_dec if self.epsilon > \\\n",
    "                            self.eps_min else self.eps_min\n",
    "    \n",
    "    def save_model(self):\n",
    "            self.q_eval.save(self.model_file)\n",
    "    \n",
    "    def load_model(self):\n",
    "            self.q_eval = load_model(self.model_file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9df22e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Box2D in c:\\users\\red_h\\envs\\sanal_ortam2\\lib\\site-packages (2.3.10)\n",
      "Requirement already satisfied: gym in c:\\users\\red_h\\envs\\sanal_ortam2\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\red_h\\envs\\sanal_ortam2\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\red_h\\envs\\sanal_ortam2\\lib\\site-packages (from gym) (1.22.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement 'box2d_py-2.3.8-cp38-cp38m-win_amd64.whl' looks like a filename, but the file does not exist\n",
      "ERROR: box2d_py-2.3.8-cp38-cp38m-win_amd64.whl is not a supported wheel on this platform.\n"
     ]
    }
   ],
   "source": [
    "!pip install Box2D\n",
    "!pip3 install  gym\n",
    "#!pip install gym[all]\n",
    "!pip install box2d_py-2.3.8-cp38-cp38m-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "487f41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import utils\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5305d497",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym.envs.box2d' has no attribute 'LunarLander'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17636/2005479252.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_eager_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LunarLander-v2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mn_games\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Envs\\sanal_ortam2\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Envs\\sanal_ortam2\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Making new env: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Envs\\sanal_ortam2\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Envs\\sanal_ortam2\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gym.envs.box2d' has no attribute 'LunarLander'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        env = gym.make(\"LunarLander-v2\")\n",
    "        lr = 0.001\n",
    "        n_games = 250\n",
    "        agent = Agent(gamma= 0.99, epsilon = 1.0, lr=lr,\n",
    "                     input_dims = env.observation_space.shape[0],\n",
    "                     n_actions = env.action_space.n, mem_size = 1000000, batch_size = 64,\n",
    "                     epsilon_end = 0.01)\n",
    "        \n",
    "        scores = []\n",
    "        eps_history = []\n",
    "        \n",
    "        for i in range(n_games):\n",
    "            done = False\n",
    "            score = 0\n",
    "            observation = env.reset()\n",
    "            while not done:\n",
    "                action = agent.choose_action(observation)\n",
    "                observation_ , reward, done, info = env.step()\n",
    "                score += reward\n",
    "                agent.store_transition(observation, action, reward, observation_, done)\n",
    "                observation = observation_\n",
    "                agent.learn()\n",
    "                \n",
    "            eps_history.append(agent.epsilon)\n",
    "            scores.append(score)\n",
    "            \n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print(\"episode: \", i , \"score %.2f\" % score,\n",
    "                 \"average_score %.2f\" % avg_score,\n",
    "                 \"epsilon %.2f\" %agent.epsilon)\n",
    "            \n",
    "            filename = \"lunalander_tf2.png\"\n",
    "            x = [i + 1 for i in range(n_games)]\n",
    "            utils.plot_learning_curve(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6551b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
